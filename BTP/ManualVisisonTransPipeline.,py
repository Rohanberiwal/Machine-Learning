import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import cv2
import numpy as np
import os
import cv2
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import zipfile
import os
import concurrent.futures
from sklearn.cluster import MiniBatchKMeans
from multiprocessing import Pool, cpu_count

def extract_zip(zip_path, extract_to='/content'):
    if not os.path.exists(zip_path):
        print(f"Error: The file {zip_path} does not exist!")
        return

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            print("Extracting files...")
            zip_ref.extractall(extract_to)
            print(f"Files successfully extracted to {extract_to}")
    except zipfile.BadZipFile:
        print("Error: The file is not a valid zip file.")

zip_file_path = "/content/Histopathology images.zip"
extract_zip(zip_file_path, extract_to='/content')


def append_images_to_list(folder_path):
    image_list = []
    supported_formats = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff')
    if not os.path.isdir(folder_path):
        print(f"The path '{folder_path}' is not a valid directory.")
        return []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith(supported_formats):
            image_list.append(os.path.join(folder_path, filename))
    return image_list

folder_path  = "/content/Histopathology images"
image_paths  = append_images_to_list(folder_path)
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import os

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

main_output_folder = "/content/Superpixel_Thing"
os.makedirs(main_output_folder, exist_ok=True)

def superpixel_segmentation(image_path, n_clusters=50):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    h, w, c = image.shape

    X = np.zeros((h * w, 5), dtype=np.float32)
    for i in range(h):
        for j in range(w):
            L, A, B = image[i, j]
            X[i * w + j] = [L, A, B, i / h, j / w]

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    segmented_image = labels.reshape(h, w)

    output = np.zeros((h, w, 3), dtype=np.uint8)
    colors = np.random.randint(0, 255, (n_clusters, 3))
    for i in range(h):
        for j in range(w):
            output[i, j] = colors[segmented_image[i, j]]

    filename = os.path.basename(image_path)
    image_name, ext = os.path.splitext(filename)
    image_folder = os.path.join(main_output_folder, image_name)
    os.makedirs(image_folder, exist_ok=True)

    save_path = os.path.join(image_folder, f"segmented_{filename}")
    cv2.imwrite(save_path, output)
    plt.figure(figsize=(10, 5))
    plt.imshow(output)
    plt.axis("off")
    plt.title(f"Superpixel Segmentation: {filename}")
    plt.show()

    print(f"Saved in: {image_folder}")

"""
for image_path in image_paths:
    superpixel_segmentation(image_path, n_clusters=100)
"""

def count_superpixel_labels(segmented_folder):
    """Counts the unique labels in each segmented superpixel image."""
    for image_folder in os.listdir(segmented_folder):
        folder_path = os.path.join(segmented_folder, image_folder)
        if os.path.isdir(folder_path):
            for filename in os.listdir(folder_path):
                if filename.startswith("segmented_") and filename.lower().endswith(('.jpg', '.png', '.jpeg')):
                    image_path = os.path.join(folder_path, filename)
                    segmented_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

                    if segmented_image is None:
                        print(f"Error reading {image_path}")
                        continue

                    unique_labels = np.unique(segmented_image)
                    print(f"{filename}: Found {len(unique_labels)} unique labels")

superpixel_folder = "/content/Superpixel_Thing"
count_superpixel_labels(superpixel_folder)
print("end ")

def print_image_sizes(segmented_folder):
    for folder in os.listdir(segmented_folder):
        folder_path = os.path.join(segmented_folder, folder)
        if os.path.isdir(folder_path):
            for filename in os.listdir(folder_path):
                if filename.startswith("segmented_") and filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_path = os.path.join(folder_path, filename)
                    img = cv2.imread(image_path)
                    if img is not None:
                        print(f"{filename}: {img.shape}")
                    else:
                        print(f"Error reading {image_path}")

superpixel_folder = "/content/Superpixel_Thing"
print_image_sizes(superpixel_folder)
print("end")

def process_images_parallel(image_folder, n_clusters=50):
    image_paths = [os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.lower().endswith(('.jpg', '.png', '.jpeg'))]
    with Pool(cpu_count()) as pool:
        results = pool.starmap(superpixel_segmentation, [(img, n_clusters) for img in image_paths])
    print("\n".join([res for res in results if res]))

def count_superpixel_labels(segmented_folder):
    def process_image(image_path):
        segmented_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if segmented_image is None:
            return f"Error reading {image_path}"
        return f"{os.path.basename(image_path)}: {len(np.unique(segmented_image))} unique labels"

    image_paths = [os.path.join(root, file) for root, _, files in os.walk(segmented_folder) for file in files if file.startswith("segmented_") and file.lower().endswith(('.jpg', '.png', '.jpeg'))]
    with Pool(cpu_count()) as pool:
        results = pool.map(process_image, image_paths)
    print("\n".join(results))

def print_image_sizes(segmented_folder):
    def get_size(image_path):
        img = cv2.imread(image_path)
        return f"{os.path.basename(image_path)}: {img.shape}" if img is not None else f"Error reading {image_path}"

    image_paths = [os.path.join(root, file) for root, _, files in os.walk(segmented_folder) for file in files if file.startswith("segmented_") and file.lower().endswith(('.jpg', '.png', '.jpeg'))]
    with Pool(cpu_count()) as pool:
        results = pool.map(get_size, image_paths)
    print("\n".join(results))

image_folder = "/content"
segmented_folder = "/content/Superpixel_Thing"

process_images_parallel(image_folder, n_clusters=100)
count_superpixel_labels(segmented_folder)
print_image_sizes(segmented_folder)

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torch.optim as optim

class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)

    def forward(self, x):
        B, C, H, W = x.shape
        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        x = x.contiguous().view(B, C, -1, self.patch_size, self.patch_size)
        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B, -1, C * self.patch_size * self.patch_size)
        return self.proj(x)

class PositionalEncoding(nn.Module):
    def __init__(self, num_patches, embed_dim):
        super().__init__()
        self.pos_embedding = torch.zeros(1, num_patches + 1, embed_dim)
        positions = torch.arange(num_patches + 1, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))
        self.pos_embedding[:, :, 0::2] = torch.sin(positions * div_term)
        self.pos_embedding[:, :, 1::2] = torch.cos(positions * div_term)

    def forward(self, x):
        return x + self.pos_embedding.to(x.device)

class LayerNorm(nn.Module):
    def __init__(self, embed_dim, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(embed_dim))
        self.beta = nn.Parameter(torch.zeros(embed_dim))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, x):
        B, N, D = x.shape
        Q = self.W_q(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.W_k(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.W_v(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_output = torch.matmul(attn_weights, V)
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, N, D)
        return self.W_o(attn_output)

class FeedForward(nn.Module):
    def __init__(self, embed_dim, mlp_dim):
        super().__init__()
        self.fc1 = nn.Linear(embed_dim, mlp_dim)
        self.fc2 = nn.Linear(mlp_dim, embed_dim)

    def forward(self, x):
        return self.fc2(F.gelu(self.fc1(x)))

class TransformerEncoder(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_dim):
        super().__init__()
        self.norm1 = LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)
        self.norm2 = LayerNorm(embed_dim)
        self.ffn = FeedForward(embed_dim, mlp_dim)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, depth=12, mlp_dim=3072):
        super().__init__()
        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
        num_patches = (img_size // patch_size) ** 2
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))
        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim),
            num_layers=depth
        )
        self.norm = nn.LayerNorm(embed_dim)

        self.reconstruction_head = nn.Linear(embed_dim, patch_size * patch_size * in_channels)

    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x).flatten(2).transpose(1, 2)
        cls_tokens = self.class_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        x = x + self.pos_embedding
        x = self.transformer(x)
        x = self.reconstruction_head(x[:, 1:])  # Skip class token
        return x.view(B, 3, 224, 224)  # Reshape back to image shape

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

class ImageDataset(Dataset):
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert("RGB")
        img = self.transform(img) if self.transform else img
        return img

dataset = ImageDataset(image_paths, transform)
dataloader = DataLoader(dataset, batch_size=2, shuffle=False)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = VisionTransformer().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)


import matplotlib.pyplot as plt

num_epochs = 100
train_losses = []
accuracies = []

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    for batch in dataloader:
        batch = batch.to(device)
        optimizer.zero_grad()

        reconstructed = model(batch)
        loss = criterion(reconstructed, batch)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        predicted = torch.argmax(reconstructed, dim=1)
        correct += (predicted == batch.argmax(dim=1)).sum().item()
        total += batch.size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total if total > 0 else 0

    train_losses.append(avg_loss)
    accuracies.append(accuracy)

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

print("Checkpoint cleared")

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_losses, label="Training Loss", marker="o")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss vs Epoch")
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), accuracies, label="Training Accuracy", marker="o", color="green")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Epoch")
plt.legend()
plt.grid()
plt.show()
